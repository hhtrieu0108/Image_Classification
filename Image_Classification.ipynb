{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7ace9815-cc0a-4d58-a89b-214e06dbbcfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/00906bc3-f060-4ce4-99ae-6d1e9acd5429\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/93c0d751-bf92-49b6-8410-14033e5ddbbf\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import kfp\n",
    "from kfp import dsl\n",
    "import kfp.components as components\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import NamedTuple\n",
    "def get_data_batch() -> NamedTuple('Outputs', [('datapoints_training', float),('datapoints_test', float),('dataset_version', str)]):\n",
    "    \"\"\"\n",
    "    Function to get dataset and load it to minio bucket\n",
    "    \"\"\"\n",
    "    print(\"getting data\")\n",
    "    from tensorflow import keras\n",
    "    from minio import Minio\n",
    "    import numpy as np\n",
    "    import json\n",
    "\n",
    "    minio_client = Minio(\n",
    "        \"192.168.1.235:9000\",\n",
    "        access_key=\"minioadmin\",\n",
    "        secret_key=\"minioadmin\",\n",
    "        secure=False\n",
    "    )\n",
    "    minio_bucket = \"mlpipeline\"\n",
    "    from tensorflow.keras.datasets import mnist\n",
    "    import numpy as np\n",
    "    \n",
    "    def load_data():\n",
    "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "        return (x_train, y_train), (x_test, y_test)\n",
    "    \n",
    "    (x_train, y_train), (x_test, y_test) = load_data()\n",
    "\n",
    "    x_predict = x_test[:15]\n",
    "    x_test = x_test[15:]\n",
    "    y_test = y_test[15:]\n",
    "    \n",
    "    x_train = x_train[:20000]\n",
    "    y_train = y_train[:20000]\n",
    "\n",
    "    # save to numpy file, store in Minio\n",
    "    np.save(\"tmp/x_train.npy\",x_train)\n",
    "    minio_client.fput_object(minio_bucket,\"x_train\",\"tmp/x_train.npy\")\n",
    "\n",
    "    np.save(\"tmp/y_train.npy\",y_train)\n",
    "    minio_client.fput_object(minio_bucket,\"y_train\",\"tmp/y_train.npy\")\n",
    "\n",
    "    np.save(\"tmp/x_test.npy\",x_test)\n",
    "    minio_client.fput_object(minio_bucket,\"x_test\",\"tmp/x_test.npy\")\n",
    "\n",
    "    np.save(\"tmp/y_test.npy\",y_test)\n",
    "    minio_client.fput_object(minio_bucket,\"y_test\",\"tmp/y_test.npy\")\n",
    "    \n",
    "    #save to numpy file, store in Minio tap predict\n",
    "    np.save(\"tmp/x_predict.npy\",x_predict)\n",
    "    minio_client.fput_object(minio_bucket,\"x_predict\",\"tmp/x_predict.npy\")\n",
    "\n",
    "    dataset_version = \"1.0\"\n",
    "    \n",
    "    print(f\"x_train shape: {x_train.shape}\")\n",
    "    print(f\"y_train shape: {y_train.shape}\")\n",
    "\n",
    "    print(f\"x_test shape: {x_test.shape}\")\n",
    "    print(f\"y_test shape: {y_test.shape}\")\n",
    "    \n",
    "    from collections import namedtuple\n",
    "    divmod_output = namedtuple('Outputs', ['datapoints_training', 'datapoints_test', 'dataset_version'])\n",
    "    return [float(x_train.shape[0]),float(x_test.shape[0]),dataset_version]\n",
    "    \n",
    "    \n",
    "def reshape_data():\n",
    "    \"\"\"\n",
    "    Reshape the data for model building\n",
    "    \"\"\"\n",
    "    print(\"reshaping data\")\n",
    "    \n",
    "    from minio import Minio\n",
    "    import numpy as np\n",
    "\n",
    "    minio_client = Minio(\n",
    "        \"192.168.1.235:9000\",\n",
    "        access_key=\"minioadmin\",\n",
    "        secret_key=\"minioadmin\",\n",
    "        secure=False\n",
    "    )\n",
    "    minio_bucket = \"mlpipeline\"\n",
    "\n",
    "    # load data from minio\n",
    "    minio_client.fget_object(minio_bucket,\"x_train\",\"/tmp/x_train.npy\")\n",
    "    x_train = np.load(\"/tmp/x_train.npy\")\n",
    "    \n",
    "    minio_client.fget_object(minio_bucket,\"x_test\",\"/tmp/x_test.npy\")\n",
    "    x_test = np.load(\"/tmp/x_test.npy\")\n",
    "    \n",
    "    #get data predict from minio\n",
    "    minio_client.fget_object(minio_bucket,\"x_predict\",\"/tmp/x_predict.npy\")\n",
    "    x_predict = np.load(\"/tmp/x_predict.npy\")\n",
    "    \n",
    "    # reshaping the data\n",
    "    x_train = x_train.reshape(-1,28,28,1)\n",
    "    x_test = x_test.reshape(-1,28,28,1)\n",
    "    x_predict = x_predict.reshape(-1,28,28,1)\n",
    "    \n",
    "    # normalizing the data\n",
    "    # each pixel has a value between 0-255. Here we divide by 255, to get values from 0-1\n",
    "    x_train = x_train / 255\n",
    "    x_test = x_test / 255\n",
    "    x_predict = x_predict/255\n",
    "    \n",
    "    # save data from minio\n",
    "    np.save(\"/tmp/x_train.npy\",x_train)\n",
    "    minio_client.fput_object(minio_bucket,\"x_train\",\"/tmp/x_train.npy\")\n",
    "    \n",
    "    np.save(\"/tmp/x_test.npy\",x_test)\n",
    "    minio_client.fput_object(minio_bucket,\"x_test\",\"/tmp/x_test.npy\")\n",
    "    \n",
    "    # save data predict from minio\n",
    "    np.save(\"/tmp/x_predict.npy\",x_predict)\n",
    "    minio_client.fput_object(minio_bucket,\"x_predict\",\"/tmp/x_predict.npy\")\n",
    "\n",
    "def model_building(\n",
    "    no_epochs:int = 5,\n",
    "    optimizer: str = \"adam\"\n",
    ") -> NamedTuple('Output', [('mlpipeline_ui_metadata', 'UI_metadata'),('mlpipeline_metrics', 'Metrics')]):\n",
    "    \"\"\"\n",
    "    Build the model with Keras API\n",
    "    Export model parameters\n",
    "    \"\"\"\n",
    "    from tensorflow import keras\n",
    "    import tensorflow as tf\n",
    "    from minio import Minio\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.metrics import plot_confusion_matrix\n",
    "    minio_client = Minio(\n",
    "        \"192.168.1.235:9000\",\n",
    "        access_key=\"minioadmin\",\n",
    "        secret_key=\"minioadmin\",\n",
    "        secure=False\n",
    "    )\n",
    "    minio_bucket = \"mlpipeline\"\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Conv2D(64, (3, 3), activation='relu',input_shape=(28,28,1)))\n",
    "    model.add(keras.layers.MaxPool2D(2, 2))\n",
    "\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(512, activation='relu'))\n",
    "\n",
    "    model.add(keras.layers.Dense(310, activation='relu'))\n",
    "\n",
    "    model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "    #show model summary - how it looks\n",
    "    stringlist = []\n",
    "    model.summary(print_fn=lambda x: stringlist.append(x))\n",
    "    metric_model_summary = \"\\n\".join(stringlist)\n",
    "    \n",
    "    #compile the model - we want to have a binary outcome\n",
    "    model.compile(optimizer=optimizer,\n",
    "            loss=\"sparse_categorical_crossentropy\",\n",
    "            metrics=['accuracy'])\n",
    "    \n",
    "    minio_client.fget_object(minio_bucket,\"x_train\",\"/tmp/x_train.npy\")\n",
    "    x_train = np.load(\"/tmp/x_train.npy\")\n",
    "    \n",
    "    minio_client.fget_object(minio_bucket,\"y_train\",\"/tmp/y_train.npy\")\n",
    "    y_train = np.load(\"/tmp/y_train.npy\")\n",
    "    \n",
    "    #fit the model and return the history while training\n",
    "    history = model.fit(\n",
    "    x=x_train,\n",
    "    y=y_train,\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    )\n",
    "    \n",
    "    minio_client.fget_object(minio_bucket,\"x_test\",\"/tmp/x_test.npy\")\n",
    "    x_test = np.load(\"/tmp/x_test.npy\")\n",
    "    \n",
    "    minio_client.fget_object(minio_bucket,\"y_test\",\"/tmp/y_test.npy\")\n",
    "    y_test = np.load(\"/tmp/y_test.npy\")\n",
    "\n",
    "    # Test the model against the test dataset\n",
    "    # Returns the loss value & metrics values for the model in test mode.\n",
    "    model_loss, model_accuracy = model.evaluate(x=x_test,y=y_test)\n",
    "    \n",
    "    # Confusion Matrix\n",
    "\n",
    "    # Generates output predictions for the input samples.\n",
    "    test_predictions = model.predict(x=x_test)\n",
    "\n",
    "    # Returns the indices of the maximum values along an axis.\n",
    "    test_predictions = np.argmax(test_predictions,axis=1) \n",
    "\n",
    "    # generate confusion matrix\n",
    "    confusion_matrix = tf.math.confusion_matrix(labels=y_test,predictions=test_predictions)\n",
    "    confusion_matrix = confusion_matrix.numpy()\n",
    "    \n",
    "    minio_client.fget_object(minio_bucket,\"x_predict\",\"tmp/x_predict.npy\")\n",
    "    image = np.load('tmp/x_predict.npy')\n",
    "    image_predict = model.predict(image)\n",
    "    list_result=[]\n",
    "    for i in range(15):\n",
    "        show_image = plt.imshow(image[i]*255,cmap=plt.get_cmap('gray'),vmin=0,vmax=1)\n",
    "        plt.savefig(f'tmp/image_{i}.png', dpi = 200)\n",
    "        minio_client.fput_object(minio_bucket,f\"image_{i}.png\",f\"tmp/image_{i}.png\")\n",
    "        index_value = max(image_predict[i])\n",
    "        for j in range(10):\n",
    "            if image_predict[i][j] == index_value:\n",
    "                list_result.append(j)\n",
    "                \n",
    "    print(list_result)\n",
    "    sr_listresult = pd.Series(list_result)\n",
    "    print(sr_listresult)\n",
    "    np.save(\"/tmp/list_result.npy\", list_result)\n",
    "    minio_client.fput_object(minio_bucket,\"list_result\",\"/tmp/list_result.npy\")\n",
    "    \n",
    "    \n",
    "    vocab = list(np.unique(y_test))\n",
    "    data = []\n",
    "    for target_index, target_row in enumerate(confusion_matrix):\n",
    "        for predicted_index, count in enumerate(target_row):\n",
    "            data.append((vocab[target_index], vocab[predicted_index], count))\n",
    "\n",
    "    df_cm = pd.DataFrame(data, columns=['target', 'predicted', 'count'])\n",
    "    cm_csv = df_cm.to_csv(header=False, index=False)\n",
    "\n",
    "    metadata = {\n",
    "        \"outputs\": [\n",
    "            {\n",
    "                \"type\": \"confusion_matrix\",\n",
    "                \"format\": \"csv\",\n",
    "                \"schema\": [\n",
    "                    {'name': 'target', 'type': 'CATEGORY'},\n",
    "                    {'name': 'predicted', 'type': 'CATEGORY'},\n",
    "                    {'name': 'count', 'type': 'NUMBER'},\n",
    "                ],\n",
    "                \"target_col\" : \"actual\",\n",
    "                \"predicted_col\" : \"predicted\",\n",
    "                \"source\": cm_csv,\n",
    "                \"storage\": \"inline\",\n",
    "                \"labels\": [0,1,2,3,4,5,6,7,8,9]\n",
    "            },\n",
    "            {\n",
    "                'storage': 'inline',\n",
    "                'source': '''# Model Overview\n",
    "## Model Summary\n",
    "\n",
    "```\n",
    "{}\n",
    "```\n",
    "\n",
    "## Model Performance\n",
    "\n",
    "**Accuracy**: {}\n",
    "**Loss**: {}\n",
    "\n",
    "'''.format(metric_model_summary,model_accuracy,model_loss),\n",
    "                'type': 'markdown',\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    metrics = {\n",
    "      'metrics': [{\n",
    "          'name': 'model_accuracy',\n",
    "          'numberValue':  float(model_accuracy),\n",
    "          'format' : \"PERCENTAGE\"\n",
    "        },{\n",
    "          'name': 'model_loss',\n",
    "          'numberValue':  float(model_loss),\n",
    "          'format' : \"PERCENTAGE\"\n",
    "        }]}\n",
    "\n",
    "    ### Save model to minIO\n",
    "    model.save('/tmp/model_detect.h5')\n",
    "    minio_client.fput_object(minio_bucket,\"model.h5\",\"/tmp/model_detect.h5\")\n",
    "    \n",
    "    from minio import Minio\n",
    "    import os\n",
    "\n",
    "    minio_client = Minio(\n",
    "            \"192.168.1.235:9000\",\n",
    "            access_key=\"minioadmin\",\n",
    "            secret_key=\"minioadmin\",\n",
    "            secure=False\n",
    "        )\n",
    "    minio_bucket = \"mlpipeline\"\n",
    "\n",
    "\n",
    "    import glob\n",
    "\n",
    "    def upload_local_directory_to_minio(local_path, bucket_name, minio_path):\n",
    "        assert os.path.isdir(local_path)\n",
    "\n",
    "        for local_file in glob.glob(local_path + '/**'):\n",
    "            local_file = local_file.replace(os.sep, \"/\") # Replace \\ with / on Windows\n",
    "            if not os.path.isfile(local_file):\n",
    "                upload_local_directory_to_minio(\n",
    "                    local_file, bucket_name, minio_path + \"/\" + os.path.basename(local_file))\n",
    "            else:\n",
    "                remote_path = os.path.join(\n",
    "                    minio_path, local_file[1 + len(local_path):])\n",
    "                remote_path = remote_path.replace(\n",
    "                    os.sep, \"/\")  # Replace \\ with / on Windows\n",
    "                minio_client.fput_object(bucket_name, remote_path, local_file)\n",
    "    \n",
    "    print(\"Saved model to minIO\")\n",
    "    \n",
    "    from collections import namedtuple\n",
    "    output = namedtuple('output', ['mlpipeline_ui_metadata', 'mlpipeline_metrics'])\n",
    "    return output(json.dumps(metadata),json.dumps(metrics))\n",
    "\n",
    "\n",
    "    \n",
    "comp_get_data_batch = components.create_component_from_func(get_data_batch,base_image=\"public.ecr.aws/j1r0q0g6/notebooks/notebook-servers/jupyter-tensorflow-full:v1.5.0\")\n",
    "comp_reshape_data = components.create_component_from_func(reshape_data,base_image=\"public.ecr.aws/j1r0q0g6/notebooks/notebook-servers/jupyter-tensorflow-full:v1.5.0\")\n",
    "comp_model_building = components.create_component_from_func(model_building,base_image=\"public.ecr.aws/j1r0q0g6/notebooks/notebook-servers/jupyter-tensorflow-full:v1.5.0\")\n",
    "\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name='digits-recognizer-pipeline',\n",
    "    description='Detect digits'\n",
    ")\n",
    "def output_test(no_epochs,optimizer):\n",
    "    step1 = comp_get_data_batch()\n",
    "    \n",
    "    step2 = comp_reshape_data()\n",
    "    step2.after(step1)\n",
    "    \n",
    "    step3 = comp_model_building(no_epochs,optimizer)\n",
    "    step3.after(step2)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    client = kfp.Client()\n",
    "\n",
    "    arguments = {\n",
    "        \"no_epochs\" : 1,\n",
    "        \"optimizer\": \"adam\"\n",
    "    }\n",
    "\n",
    "    run_directly = 1\n",
    "    \n",
    "    if (run_directly == 1):\n",
    "        client.create_run_from_pipeline_func(output_test,arguments=arguments,experiment_name=\"test\")\n",
    "    else:\n",
    "        kfp.compiler.Compiler().compile(pipeline_func=output_test,package_path='output_test.yaml')\n",
    "        client.upload_pipeline_version(pipeline_package_path='output_test.yaml',pipeline_version_name=\"0.4\",pipeline_name=\"pipeline test\",description=\"just for testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c1fe97bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 124ms/step\n",
      "The predicted class is: 3\n"
     ]
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "model = keras.models.load_model('model1.keras')\n",
    "user_input = r\"img1.jpg\"\n",
    "if user_input.endswith('.txt'):\n",
    "    with open(user_input, 'r') as file:\n",
    "        data = file.read().split(',')\n",
    "    # Convert the input data to numpy array\n",
    "    input_data = np.array(data, dtype=np.float32).reshape(1, 28, 28, 1) / 255.0  # Assuming 28x28 grayscale image\n",
    "else:\n",
    "    # Assume input is a path to an image file\n",
    "    img = tf.keras.preprocessing.image.load_img(user_input, color_mode='grayscale', target_size=(28, 28))\n",
    "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    input_data = np.expand_dims(img_array, axis=0) / 255.0  # Normalize the image data\n",
    "\n",
    "# Perform prediction using the loaded model\n",
    "predictions = model.predict(input_data)\n",
    "\n",
    "# Get the predicted class (assuming a classification task)\n",
    "predicted_class = np.argmax(predictions)\n",
    "\n",
    "print(f\"The predicted class is: {predicted_class}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
